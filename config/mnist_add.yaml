trials: 5
results_dir: results/mnist_add


shared_params:
  # Dataset Configuration
  dataset_config:
    dataset: mnist_add
    root_dir: data/
    num_workers: 8
    batch_size: 2048
    num_operands: 12
    selected_digits:
      - [0,1,2]
      - [0,1,2]
      - [0,1,2]
      - [0,1,2]
      - [0,1,2,3,4]
      - [0,1,2,3,4]
      - [0,1,2,3,4]
      - [0,1,2,3,4]
      - [0,1,2,3,4,5,6,7,8,9]
      - [0,1,2,3,4,5,6,7,8,9]
      - [0,1,2,3,4,5,6,7,8,9]
      - [0,1,2,3,4,5,6,7,8,9]
    threshold_labels: 30
    noise_level: 0.0
    train_dataset_size: 10000
    sampling_percent: 1
    sampling_groups: True
    test_subsampling: 1
    weight_loss: True


  # Intervention Parameters
  intervention_config:
    competence_levels: [1, 0]
    intervention_freq: 1
    intervention_batch_size: 2048
    intervention_policies:
      - policy: "random"
        group_level: True
        use_prior: True  # This will make the random policy use the learnt IntCEM prior!!!
      - policy: "random"
        group_level: True
        use_prior: False
      - policy: "coop"
        group_level: True
        use_prior: False
      - policy: "behavioural_cloning"
        group_level: True
        use_prior: False
      - policy: "optimal_greedy"
        group_level: True
        use_prior: False
      - policy: "global_val_error"
        group_level: True
        use_prior: False
      - policy: "global_val_improvement"
        group_level: True
        use_prior: False

  # Representation metrics
  # Change to False if you want representation metrics to be included in the
  # evaluation (may significantly increase experiment times)
  skip_repr_evaluation: True
  top_k_accuracy: null
  save_model: True
  patience: 5
  emb_size: 16
  extra_dims: 0
  concept_loss_weight: 10
  learning_rate: 0.001
  weight_decay: 0.000004
  c_extractor_arch: mnist_extractor
  optimizer: sgd
  max_epochs: 300
  bool: False
  early_stopping_monitor: val_loss
  early_stopping_mode: min
  early_stopping_delta: 0.0
  momentum: 0.9
  sigmoidal_prob: False
  training_intervention_prob: 0.25
  c2y_layers: [128, 128]
  use_task_class_weights: True
  weight_loss: True
  check_val_every_n_epoch: 2

runs:
  - architecture: 'ConceptEmbeddingModel'
    run_name: "CEM"
    sigmoidal_prob: True
    training_intervention_prob: 0.25
    embedding_activation: "leakyrelu"

  - architecture: "IntAwareConceptEmbeddingModel"
    run_name: "IntCEM_intervention_weight_{intervention_weight}_intervention_task_discount_{intervention_task_discount}"
    training_intervention_prob: 0.25
    intervention_weight: [0.1, 1, 5]
    intervention_task_discount: [1.1, 1.5]
    use_concept_groups: True
    int_model_use_bn: True
    int_model_layers: [128, 128, 64, 64]
    embedding_activation: "leakyrelu"
    max_horizon: 6
    horizon_rate: 1.005
    gradient_clip_val: 100
    grid_variables:
        - intervention_task_discount
        - intervention_weight
    grid_search_mode: exhaustive

  - architecture: 'ConceptBottleneckModel'
    run_name: "CBM_Sigmoid"
    concept_loss_weight: 5
    max_epochs: 500
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True

  - architecture: 'ConceptBottleneckModel'
    run_name: "CBM_Logit"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: False
    max_epochs: 500

  - architecture: 'SequentialConceptBottleneckModel'
    run_name: "CBM_Seq"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True
    c2y_max_epochs: 200

  - architecture: 'IndependentConceptBottleneckModel'
    run_name: "CBM_Ind"
    embedding_activation: "leakyrelu"
    bool: False
    extra_dims: 0
    sigmoidal_extra_capacity: False
    sigmoidal_prob: True
    max_epochs: 300